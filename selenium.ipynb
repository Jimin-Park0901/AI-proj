{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic -> \"excavator_data.xlsx\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detail한 정보는 없음. (Just Model and Info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'Load More' button. Exiting loop.\n",
      "Data saved to excavator_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urlparse\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to set up the WebDriver with anti-detection measures\n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    # Randomized User-Agent\n",
    "    options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT {random.randint(6, 10)}.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(90, 110)}.0.4472.{random.randint(100, 200)} Safari/537.36\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "# Function to add random delay\n",
    "def random_delay(min_seconds=1, max_seconds=3):\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "# Main scraping logic\n",
    "def scrape_excavator_data():\n",
    "    url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "    driver = setup_driver()\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Create Excel workbook\n",
    "    workbook = openpyxl.Workbook()\n",
    "    sheet = workbook.active\n",
    "    sheet.title = \"Excavator Data\"\n",
    "    sheet.append([\"Model\", \"Info\", \"Details\"])\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Extract models and info\n",
    "            model_elements = driver.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "            info_elements = driver.find_elements(By.CSS_SELECTOR, \".product_info.en_plain_l\")\n",
    "            \n",
    "            if len(model_elements) != len(info_elements):\n",
    "                print(\"Mismatch in data lengths. Exiting.\")\n",
    "                break\n",
    "            \n",
    "            for model, info in zip(model_elements, info_elements):\n",
    "                # Simulate human-like delay\n",
    "                random_delay()\n",
    "                sheet.append([model.text, info.text, \"Details Placeholder\"])  # Placeholder for details\n",
    "                \n",
    "            # Click 'Load More' button\n",
    "            try:\n",
    "                load_more_button = driver.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "                ActionChains(driver).move_to_element(load_more_button).perform()\n",
    "                random_delay()\n",
    "                load_more_button.click()\n",
    "                random_delay(2, 4)  # Longer delay for content load\n",
    "            except NoSuchElementException:\n",
    "                print(\"No 'Load More' button. Exiting loop.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "    \n",
    "    # Save data to Excel\n",
    "    file_name = \"excavator_data.xlsx\"\n",
    "    workbook.save(file_name)\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "    \n",
    "    # Close browser\n",
    "    driver.quit()\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_excavator_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type1 (첫번째에는 잘 작동됐는데 captcha문제로 지금은 안됨.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Cannot convert                               0                             1   \\\n",
      "Classification  Operating Weight  Bucket Capacity (SAE HEAPED)   \n",
      "DX89R-7                 9,518 kg                       0.28 m³   \n",
      "\n",
      "                                           2             3             4   \\\n",
      "Classification  Engine Power (SAE j1349, net)  Engine Model  Travel Speed   \n",
      "DX89R-7                   48.5/2,100 (kW/rpm)   Develon D24  2.7/4.7 km/h   \n",
      "\n",
      "                                5                   6                    7   \\\n",
      "Classification  Max. Digging Reach  Max. Digging Depth  Max. Digging Height   \n",
      "DX89R-7                   7,015 mm            4,110 mm             7,030 mm   \n",
      "\n",
      "                            8              9               10  \n",
      "Classification  Overall Length  Overall Width  Overall Height  \n",
      "DX89R-7               6,430 mm       2,250 mm        2,657 mm   to Excel\n",
      "Data successfully saved to excavator_data_with_details.xlsx\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to search Google\n",
    "def search_google(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    # Set up the Chrome WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")  # Run in headless mode (without opening a browser window)\n",
    "    # options.add_argument(\"--disable-gpu\")\n",
    "    # options.add_argument(\"--no-sandbox\")\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(search_url)\n",
    "    \n",
    "    # Wait for results to load\n",
    "    time.sleep(3)  # Wait for the page to load, adjust if needed\n",
    "    \n",
    "    return driver\n",
    "\n",
    "# Function to get the first search result\n",
    "def get_first_search_result(url, model):\n",
    "    query = extract_domain(url) + \"+\" + model\n",
    "    driver = search_google(query)\n",
    "    \n",
    "    # Find the first search result using the href attribute\n",
    "    try:\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        link = first_result.get_attribute('href')  # Extract the href attribute (the URL)\n",
    "        return link\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        driver.quit()  # Close the browser when done\n",
    "\n",
    "# Initialize the WebDriver for the first website\n",
    "options = Options()\n",
    "b = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open the target URL\n",
    "url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "b.get(url)\n",
    "\n",
    "# Create a new Excel workbook and sheet\n",
    "workbook = openpyxl.Workbook()\n",
    "table = workbook.active\n",
    "element = b.find_element(By.CLASS_NAME, \"en_plain_b\")  # Get the WebElement\n",
    "table.title = element.text + \"data\"  # Concatenate its text content with \"data\"\n",
    "\n",
    "# Add headers to the Excel sheet\n",
    "table.append([\"Model\", \"Info\", \"Details\"])\n",
    "\n",
    "# Extract data and insert into the Excel sheet\n",
    "while True:\n",
    "    try:\n",
    "        # Find elements for model names and info\n",
    "        model_elements = b.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "        info_elements = b.find_elements(By.CSS_SELECTOR, \".product_info.en_plain_l\")\n",
    "\n",
    "        # Check if both lists have the same length\n",
    "        if len(model_elements) != len(info_elements):\n",
    "            print(\"Mismatch in number of model and info elements. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        # Add data to the Excel sheet\n",
    "        for model, info in zip(model_elements, info_elements):\n",
    "            # Get the first search result for the model\n",
    "            first_result = get_first_search_result(url, model.text)\n",
    "\n",
    "            # Extract details from the first result\n",
    "            try:\n",
    "                tables = pd.read_html(first_result)\n",
    "                combined_table = pd.concat([tables[0], tables[1]], axis=1)  # Combine tables if needed\n",
    "                combined_table_transposed = combined_table.T  # Transpose the combined table\n",
    "                details = combined_table_transposed\n",
    "            except Exception as e:\n",
    "                details = \"Details not found\"\n",
    "\n",
    "            # Append model, info, and details to the sheet\n",
    "            table.append([model.text, info.text, details])\n",
    "\n",
    "        # Check if a \"Load More\" button exists\n",
    "        try:\n",
    "            load_more_button = b.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "            ActionChains(b).move_to_element(load_more_button).perform()\n",
    "            time.sleep(0.5)\n",
    "            load_more_button.click()\n",
    "            time.sleep(2)  # Wait for content to load\n",
    "        except NoSuchElementException:\n",
    "            print(\"No 'Load More' button found. Exiting loop.\")\n",
    "            break  # Exit loop if no \"Load More\" button is found\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break\n",
    "\n",
    "# Save the workbook to an Excel file\n",
    "file_name = \"excavator_data_with_details(2).xlsx\"\n",
    "workbook.save(file_name)\n",
    "print(f\"Data successfully saved to {file_name}\")\n",
    "\n",
    "# Close the WebDriver\n",
    "b.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type2 -> \"excator_data_with_link.xlsx\" (지금은 링크만 존재) (40~60min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type1의 captcha 해결 버전"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Kept failing so I checked whether if it uses correct url and apparently google blocks after certain usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'Load More' button. Exiting loop.\n",
      "Data saved to excavator_data_with_details.xlsx\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urlparse\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import openpyxl\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to set up the WebDriver with anti-detection measures\n",
    "def setup_driver(headless=False):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    # Randomized User-Agent\n",
    "    options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT {random.randint(6, 10)}.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(90, 110)}.0.4472.{random.randint(100, 200)} Safari/537.36\")\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "# Function to add random delay\n",
    "def random_delay(min_seconds=1, max_seconds=3):\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "# Function to search Google and get the first result\n",
    "def search_google(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    driver = setup_driver(headless=True)\n",
    "    driver.get(search_url)\n",
    "    random_delay(2, 4)\n",
    "    try:\n",
    "        # Find the first search result\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        link = first_result.get_attribute('href')  # Extract the href attribute (the URL)\n",
    "        return link\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Main scraping logic\n",
    "def scrape_excavator_data():\n",
    "    url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "    driver = setup_driver()\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Create Excel workbook\n",
    "    workbook = openpyxl.Workbook()\n",
    "    sheet = workbook.active\n",
    "    sheet.title = \"Excavator Data\"\n",
    "    sheet.append([\"Model\", \"Info\", \"Details\"])\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Extract models and info\n",
    "            model_elements = driver.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "            info_elements = driver.find_elements(By.CSS_SELECTOR, \".product_info.en_plain_l\")\n",
    "            \n",
    "            if len(model_elements) != len(info_elements):\n",
    "                print(\"Mismatch in data lengths. Exiting.\")\n",
    "                break\n",
    "            \n",
    "            for model, info in zip(model_elements, info_elements):\n",
    "                random_delay()\n",
    "                model_text = model.text\n",
    "                info_text = info.text\n",
    "                # Search Google for additional details\n",
    "                query = extract_domain(url) + \"+\" + model_text\n",
    "                details = search_google(query)\n",
    "                sheet.append([model_text, info_text, details])\n",
    "            \n",
    "            # Click 'Load More' button\n",
    "            try:\n",
    "                load_more_button = driver.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "                ActionChains(driver).move_to_element(load_more_button).perform()\n",
    "                random_delay()\n",
    "                load_more_button.click()\n",
    "                random_delay(2, 4)  # Longer delay for content load\n",
    "            except NoSuchElementException:\n",
    "                print(\"No 'Load More' button. Exiting loop.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "\n",
    "    # Save data to Excel\n",
    "    file_name = \"excavator_data_with_details.xlsx\"\n",
    "    workbook.save(file_name)\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "    \n",
    "    # Close browser\n",
    "    driver.quit()\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_excavator_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type3 (w or w/o info)-> \"excavator_data_with_details.xlsx\" (이렇게 되어야함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Trial1 (.to_excel) -> excavator_data_with_details(1).xlsx (100+min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y3/wg1q_3v16xn_pfr29jl68nqc0000gn/T/ipykernel_69176/4231402285.py:87: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(first_result)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 121\u001b[0m\n\u001b[1;32m    118\u001b[0m     driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 121\u001b[0m     scrape_excavator_data()\n",
      "Cell \u001b[0;32mIn[27], line 84\u001b[0m, in \u001b[0;36mscrape_excavator_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model, info \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model_elements, info_elements):\n\u001b[1;32m     83\u001b[0m     random_delay()\n\u001b[0;32m---> 84\u001b[0m     first_result \u001b[38;5;241m=\u001b[39m get_first_search_result(url,model\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m         tables \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_html(first_result)\n",
      "Cell \u001b[0;32mIn[27], line 45\u001b[0m, in \u001b[0;36mget_first_search_result\u001b[0;34m(url, model)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_first_search_result\u001b[39m(url, model):\n\u001b[1;32m     44\u001b[0m     query \u001b[38;5;241m=\u001b[39m extract_domain(url) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model\n\u001b[0;32m---> 45\u001b[0m     driver \u001b[38;5;241m=\u001b[39m search_google(query)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Find the first search result using the href attribute\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[27], line 33\u001b[0m, in \u001b[0;36msearch_google\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Randomized User-Agent\u001b[39;00m\n\u001b[1;32m     31\u001b[0m options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent=Mozilla/5.0 (Windows NT \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m6\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m90\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m110\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.0.4472.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m100\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m200\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Safari/537.36\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mService(ChromeDriverManager()\u001b[38;5;241m.\u001b[39minstall()), options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m     35\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(search_url)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m driver\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/webdriver_manager/chrome.py:40\u001b[0m, in \u001b[0;36mChromeDriverManager.install\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minstall\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 40\u001b[0m     driver_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_driver_binary_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver)\n\u001b[1;32m     41\u001b[0m     os\u001b[38;5;241m.\u001b[39mchmod(driver_path, \u001b[38;5;241m0o755\u001b[39m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m driver_path\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/webdriver_manager/core/manager.py:35\u001b[0m, in \u001b[0;36mDriverManager._get_driver_binary_path\u001b[0;34m(self, driver)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_driver_binary_path\u001b[39m(\u001b[38;5;28mself\u001b[39m, driver):\n\u001b[0;32m---> 35\u001b[0m     binary_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_manager\u001b[38;5;241m.\u001b[39mfind_driver(driver)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary_path:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m binary_path\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/webdriver_manager/core/driver_cache.py:107\u001b[0m, in \u001b[0;36mDriverCacheManager.find_driver\u001b[0;34m(self, driver)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m browser_version:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m driver_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_cache_key_driver_version(driver)\n\u001b[1;32m    108\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_metadata_content()\n\u001b[1;32m    110\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_metadata_key(driver)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/webdriver_manager/core/driver_cache.py:154\u001b[0m, in \u001b[0;36mDriverCacheManager.get_cache_key_driver_version\u001b[0;34m(self, driver)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_key_driver_version:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_key_driver_version\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m driver\u001b[38;5;241m.\u001b[39mget_driver_version_to_download()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/webdriver_manager/core/driver.py:48\u001b[0m, in \u001b[0;36mDriver.get_driver_version_to_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver_version_to_download:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver_version_to_download\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_latest_release_version()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/webdriver_manager/drivers/chrome.py:60\u001b[0m, in \u001b[0;36mChromeDriver.get_latest_release_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://googlechromelabs.github.io/chrome-for-testing/latest-patch-versions-per-build.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_client\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m---> 60\u001b[0m response_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     61\u001b[0m determined_browser_version \u001b[38;5;241m=\u001b[39m response_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget(determined_browser_version)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m determined_browser_version\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/models.py:926\u001b[0m, in \u001b[0;36mResponse.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    923\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    924\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding\n\u001b[0;32m--> 926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;66;03m# Fallback to auto-detected encoding.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_read(amt)\n\u001b[1;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urlparse\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to set up the WebDriver with anti-detection measures\n",
    "\n",
    "def search_google(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    # Randomized User-Agent\n",
    "    options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT {random.randint(6, 10)}.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(90, 110)}.0.4472.{random.randint(100, 200)} Safari/537.36\")\n",
    "        \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(search_url)\n",
    "\n",
    "    return driver\n",
    "    \n",
    "\n",
    "def random_delay(min_seconds=1, max_seconds=3):\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "def get_first_search_result(url, model):\n",
    "    query = extract_domain(url) + \"+\" + model\n",
    "    driver = search_google(query)\n",
    "    \n",
    "    # Find the first search result using the href attribute\n",
    "    try:\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        link = first_result.get_attribute('href')  # Extract the href attribute (the URL)\n",
    "        return link\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        driver.quit()  # Close the browser when done\n",
    "\n",
    "# Function to scrape excavator data\n",
    "def scrape_excavator_data():\n",
    "    url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "    driver = setup_driver()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Create Excel workbook\n",
    "    workbook = openpyxl.Workbook()\n",
    "    table = workbook.active\n",
    "    table.title = \"Excavator Data\"\n",
    "    table.append([\"Model\", \"Info\", \"Detail\"])\n",
    "\n",
    "\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Extract models and info\n",
    "            model_elements = driver.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "            info_elements = driver.find_elements(By.CSS_SELECTOR, \".product_info.en_plain_l\")\n",
    "\n",
    "            if len(model_elements) != len(info_elements):\n",
    "                print(\"Mismatch in data lengths. Exiting.\")\n",
    "                break\n",
    "\n",
    "            for model, info in zip(model_elements, info_elements):\n",
    "                random_delay()\n",
    "                first_result = get_first_search_result(url,model.text)\n",
    "\n",
    "                try:\n",
    "                    tables = pd.read_html(first_result)\n",
    "                    # detail_title_raw = tables[0]\n",
    "                    # detail_title_transposed = detail_title_raw.T\n",
    "                    # detail_title = detail_title_transposed\n",
    "\n",
    "                    detail_info = tables[1]\n",
    "                    detail_info_transposed = detail_info.T\n",
    "                    details = detail_info_transposed.to_excel(index=False, header=False)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    details = \"Details not found\"\n",
    "                \n",
    "                table.append([model.text, info.text, details])\n",
    "\n",
    "            try:\n",
    "                load_more_button = driver.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "                ActionChains(driver).move_to_element(load_more_button).perform()\n",
    "                random_delay()\n",
    "                load_more_button.click()\n",
    "                random_delay(2, 4)  # Longer delay for content load\n",
    "            except NoSuchElementException:\n",
    "                print(\"No 'Load More' button. Exiting loop.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "\n",
    "    file_name = \"excavator_data_with_details(1).xlsx\"\n",
    "    workbook.save(file_name)\n",
    "    print(f\"Data successfully saved to {file_name}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_excavator_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Trial1.1 (.to_excel)(with progressbar) -> execavator_data_with_details(1.1).xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Models:   0%|          | 0/24 [00:00<?, ?it/s]/var/folders/y3/wg1q_3v16xn_pfr29jl68nqc0000gn/T/ipykernel_69176/1054972137.py:83: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(first_result)\n",
      "Scraping Models: 100%|██████████| 24/24 [02:36<00:00,  6.51s/it]\n",
      "Scraping Models: 100%|██████████| 24/24 [02:37<00:00,  6.56s/it]\n",
      "Scraping Models: 100%|██████████| 24/24 [02:29<00:00,  6.24s/it]\n",
      "Scraping Models: 100%|██████████| 24/24 [02:26<00:00,  6.10s/it]\n",
      "Scraping Models:  71%|███████   | 17/24 [01:51<00:55,  7.87s/it]/var/folders/y3/wg1q_3v16xn_pfr29jl68nqc0000gn/T/ipykernel_69176/1054972137.py:83: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(first_result)\n",
      "Scraping Models: 100%|██████████| 24/24 [02:36<00:00,  6.53s/it]\n",
      "Scraping Models: 100%|██████████| 24/24 [02:29<00:00,  6.22s/it]\n",
      "Scraping Models: 100%|██████████| 24/24 [02:30<00:00,  6.28s/it]\n",
      "Scraping Models:   4%|▍         | 1/24 [00:11<04:31, 11.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not reach host. Are you offline?\n",
      "Data successfully saved to excavator_data_with_details(1.1).xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urlparse\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to search Google\n",
    "def search_google(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT {random.randint(6, 10)}.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(90, 110)}.0.4472.{random.randint(100, 200)} Safari/537.36\")\n",
    "        \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(search_url)\n",
    "    return driver\n",
    "\n",
    "# Add a delay to mimic human behavior\n",
    "def random_delay(min_seconds=1, max_seconds=3):\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "def get_first_search_result(url, model):\n",
    "    query = extract_domain(url) + \"+\" + model\n",
    "    driver = search_google(query)\n",
    "    \n",
    "    try:\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        link = first_result.get_attribute('href')\n",
    "        return link\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Main function to scrape excavator data\n",
    "def scrape_excavator_data():\n",
    "    url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Create Excel workbook\n",
    "    workbook = openpyxl.Workbook()\n",
    "    table = workbook.active\n",
    "    table.title = \"Excavator Data\"\n",
    "    table.append([\"Model\", \"Info\", \"Detail\"])\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            model_elements = driver.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "            info_elements = driver.find_elements(By.CSS_SELECTOR, \".product_info.en_plain_l\")\n",
    "\n",
    "            if len(model_elements) != len(info_elements):\n",
    "                print(\"Mismatch in data lengths. Exiting.\")\n",
    "                break\n",
    "\n",
    "            for model, info in tqdm(zip(model_elements, info_elements), total=len(model_elements), desc=\"Scraping Models\"):\n",
    "                random_delay()\n",
    "                first_result = get_first_search_result(url, model.text)\n",
    "\n",
    "                try:\n",
    "                    tables = pd.read_html(first_result)\n",
    "                    detail_info = tables[1]\n",
    "                    details = detail_info.to_excel(index=False, header=False)\n",
    "                except Exception as e:\n",
    "                    details = \"Details not found\"\n",
    "\n",
    "                table.append([model.text, info.text, details])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "\n",
    "    file_name = \"excavator_data_with_details(1.1).xlsx\"\n",
    "    workbook.save(file_name)\n",
    "    print(f\"Data successfully saved to {file_name}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_excavator_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Trial1.2 (.to_excel)(with progress bar)(with no info -> might need to modify the code more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urlparse\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to search Google\n",
    "def search_google(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT {random.randint(6, 10)}.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(90, 110)}.0.4472.{random.randint(100, 200)} Safari/537.36\")\n",
    "        \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(search_url)\n",
    "    return driver\n",
    "\n",
    "# Add a delay to mimic human behavior\n",
    "def random_delay(min_seconds=1, max_seconds=3):\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "def get_first_search_result(url, model):\n",
    "    query = extract_domain(url) + \"+\" + model\n",
    "    driver = search_google(query)\n",
    "    \n",
    "    try:\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        link = first_result.get_attribute('href')\n",
    "        return link\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Main function to scrape excavator data\n",
    "def scrape_excavator_data():\n",
    "    url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Create Excel workbook\n",
    "    workbook = openpyxl.Workbook()\n",
    "    table = workbook.active\n",
    "    table.title = \"Excavator Data\"\n",
    "    table.append([\"Model\", \"Info\", \"Detail\"])\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            model_elements = driver.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "\n",
    "            for model, info in tqdm(zip(model_elements), total=len(model_elements), desc=\"Scraping Models\"):\n",
    "                random_delay()\n",
    "                first_result = get_first_search_result(url, model.text)\n",
    "\n",
    "                try:\n",
    "                    tables = pd.read_html(first_result)\n",
    "                    detail_info = tables[1]\n",
    "                    details = detail_info.to_excel(index=False, header=False)\n",
    "                except Exception as e:\n",
    "                    details = \"Details not found\"\n",
    "                \n",
    "\n",
    "                table.append([model.text, details])\n",
    "\n",
    "            try:\n",
    "                load_more_button = driver.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "                ActionChains(driver).move_to_element(load_more_button).perform()\n",
    "                random_delay()\n",
    "                load_more_button.click()\n",
    "                random_delay(2, 4)  # Longer delay for content load\n",
    "            except NoSuchElementException:\n",
    "                print(\"No 'Load More' button. Exiting loop.\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "\n",
    "    file_name = \"excavator_data_with_details(1.2).xlsx\"\n",
    "    workbook.save(file_name)\n",
    "    print(f\"Data successfully saved to {file_name}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_excavator_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Trial2 (.to_string) -> excavator_data_with_details(2).xlsx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urlparse\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to set up the WebDriver with anti-detection measures\n",
    "\n",
    "def search_google(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    # Randomized User-Agent\n",
    "    options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT {random.randint(6, 10)}.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(90, 110)}.0.4472.{random.randint(100, 200)} Safari/537.36\")\n",
    "        \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(search_url)\n",
    "\n",
    "    return driver\n",
    "    \n",
    "\n",
    "def random_delay(min_seconds=1, max_seconds=3):\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "def get_first_search_result(url, model):\n",
    "    query = extract_domain(url) + \"+\" + model\n",
    "    driver = search_google(query)\n",
    "    \n",
    "    # Find the first search result using the href attribute\n",
    "    try:\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        link = first_result.get_attribute('href')  # Extract the href attribute (the URL)\n",
    "        return link\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        driver.quit()  # Close the browser when done\n",
    "\n",
    "# Function to scrape excavator data\n",
    "def scrape_excavator_data():\n",
    "    url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "    driver = setup_driver()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Create Excel workbook\n",
    "    workbook = openpyxl.Workbook()\n",
    "    table = workbook.active\n",
    "    table.title = \"Excavator Data\"\n",
    "    table.append([\"Model\", \"Info\", \"Detail\"])\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Extract models and info\n",
    "            model_elements = driver.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "            info_elements = driver.find_elements(By.CSS_SELECTOR, \".product_info.en_plain_l\")\n",
    "\n",
    "            if len(model_elements) != len(info_elements):\n",
    "                print(\"Mismatch in data lengths. Exiting.\")\n",
    "                break\n",
    "\n",
    "            for model, info in zip(model_elements, info_elements):\n",
    "                random_delay()\n",
    "                first_result = get_first_search_result(url,model.text)\n",
    "\n",
    "                try:\n",
    "                    tables = pd.read_html(first_result)\n",
    "                    # detail_title_raw = tables[0]\n",
    "                    # detail_title_transposed = detail_title_raw.T\n",
    "                    # detail_title = detail_title_transposed\n",
    "\n",
    "                    detail_info = tables[1]\n",
    "                    detail_info_transposed = detail_info.T\n",
    "                    details = detail_info_transposed.to_string(index=False, header=False)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    details = \"Details not found\"\n",
    "                \n",
    "                table.append([model.text, info.text, details])\n",
    "\n",
    "            try:\n",
    "                load_more_button = driver.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "                ActionChains(driver).move_to_element(load_more_button).perform()\n",
    "                random_delay()\n",
    "                load_more_button.click()\n",
    "                random_delay(2, 4)  # Longer delay for content load\n",
    "            except NoSuchElementException:\n",
    "                print(\"No 'Load More' button. Exiting loop.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "\n",
    "    file_name = \"excavator_data_with_details(3).xlsx\"\n",
    "    workbook.save(file_name)\n",
    "    print(f\"Data successfully saved to {file_name}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_excavator_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Trial3 () -> excavator_data_with_details(3).xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urlparse\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to set up the WebDriver with anti-detection measures\n",
    "\n",
    "def search_google(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    # Randomized User-Agent\n",
    "    options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT {random.randint(6, 10)}.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(90, 110)}.0.4472.{random.randint(100, 200)} Safari/537.36\")\n",
    "        \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(search_url)\n",
    "\n",
    "    return driver\n",
    "    \n",
    "\n",
    "def random_delay(min_seconds=1, max_seconds=3):\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "def get_first_search_result(url, model):\n",
    "    query = extract_domain(url) + \"+\" + model\n",
    "    driver = search_google(query)\n",
    "    \n",
    "    # Find the first search result using the href attribute\n",
    "    try:\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        link = first_result.get_attribute('href')  # Extract the href attribute (the URL)\n",
    "        return link\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        driver.quit()  # Close the browser when done\n",
    "\n",
    "# Function to scrape excavator data\n",
    "def scrape_excavator_data():\n",
    "    url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "    driver = setup_driver()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Create Excel workbook\n",
    "    workbook = openpyxl.Workbook()\n",
    "    table = workbook.active\n",
    "    table.title = \"Excavator Data\"\n",
    "    table.append([\"Model\", \"Info\", \"Detail\"])\n",
    "\n",
    "\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Extract models and info\n",
    "            model_elements = driver.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "            info_elements = driver.find_elements(By.CSS_SELECTOR, \".product_info.en_plain_l\")\n",
    "\n",
    "            if len(model_elements) != len(info_elements):\n",
    "                print(\"Mismatch in data lengths. Exiting.\")\n",
    "                break\n",
    "\n",
    "            for model, info in zip(model_elements, info_elements):\n",
    "                random_delay()\n",
    "                first_result = get_first_search_result(url,model.text)\n",
    "\n",
    "                try:\n",
    "                    tables = pd.read_html(first_result)\n",
    "                    # detail_title_raw = tables[0]\n",
    "                    # detail_title_transposed = detail_title_raw.T\n",
    "                    # detail_title = detail_title_transposed\n",
    "\n",
    "                    detail_info = tables[1]\n",
    "                    detail_info_transposed = detail_info.T\n",
    "                    details = detail_info_transposed(index=False, header=False)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    details = \"Details not found\"\n",
    "                \n",
    "                table.append([model.text, info.text, details])\n",
    "\n",
    "            try:\n",
    "                load_more_button = driver.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "                ActionChains(driver).move_to_element(load_more_button).perform()\n",
    "                random_delay()\n",
    "                load_more_button.click()\n",
    "                random_delay(2, 4)  # Longer delay for content load\n",
    "            except NoSuchElementException:\n",
    "                print(\"No 'Load More' button. Exiting loop.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "\n",
    "    file_name = \"excavator_data_with_details(1).xlsx\"\n",
    "    workbook.save(file_name)\n",
    "    print(f\"Data successfully saved to {file_name}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_excavator_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Trial 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urlparse\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to set up the WebDriver with anti-detection measures\n",
    "\n",
    "def search_google(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    # Randomized User-Agent\n",
    "    options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT {random.randint(6, 10)}.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(90, 110)}.0.4472.{random.randint(100, 200)} Safari/537.36\")\n",
    "        \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(search_url)\n",
    "\n",
    "    return driver\n",
    "    \n",
    "\n",
    "def random_delay(min_seconds=1, max_seconds=3):\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "def get_first_search_result(url, model):\n",
    "    query = extract_domain(url) + \"+\" + model\n",
    "    driver = search_google(query)\n",
    "    \n",
    "    # Find the first search result using the href attribute\n",
    "    try:\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        link = first_result.get_attribute('href')  # Extract the href attribute (the URL)\n",
    "        return link\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        driver.quit()  # Close the browser when done\n",
    "\n",
    "# Function to scrape excavator data\n",
    "def scrape_excavator_data():\n",
    "    url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "    driver = setup_driver()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Create Excel workbook\n",
    "    workbook = openpyxl.Workbook()\n",
    "    table = workbook.active\n",
    "    table.title = \"Excavator Data\"\n",
    "    table.append([\"Model\", \"Info\", \"Detail\"])\n",
    "\n",
    "\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Extract models and info\n",
    "            model_elements = driver.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "            info_elements = driver.find_elements(By.CSS_SELECTOR, \".product_info.en_plain_l\")\n",
    "\n",
    "            if len(model_elements) != len(info_elements):\n",
    "                print(\"Mismatch in data lengths. Exiting.\")\n",
    "                break\n",
    "\n",
    "            for model, info in zip(model_elements, info_elements):\n",
    "                random_delay()\n",
    "                first_result = get_first_search_result(url,model.text)\n",
    "\n",
    "                try:\n",
    "                    tables = pd.read_html(first_result)\n",
    "                    # detail_title_raw = tables[0]\n",
    "                    # detail_title_transposed = detail_title_raw.T\n",
    "                    # detail_title = detail_title_transposed\n",
    "\n",
    "                    detail_info = tables[1]\n",
    "                    detail_info_transposed = detail_info.T\n",
    "                    details = detail_info_transposed(index=False, header=False)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    details = \"Details not found\"\n",
    "                \n",
    "                table.append([model.text, info.text, details])\n",
    "\n",
    "            try:\n",
    "                load_more_button = driver.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "                ActionChains(driver).move_to_element(load_more_button).perform()\n",
    "                random_delay()\n",
    "                load_more_button.click()\n",
    "                random_delay(2, 4)  # Longer delay for content load\n",
    "            except NoSuchElementException:\n",
    "                print(\"No 'Load More' button. Exiting loop.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "\n",
    "    file_name = \"excavator_data_with_details(1).xlsx\"\n",
    "    workbook.save(file_name)\n",
    "    print(f\"Data successfully saved to {file_name}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_excavator_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Trial 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urlparse\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to search Google\n",
    "def search_google(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT {random.randint(6, 10)}.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(90, 110)}.0.4472.{random.randint(100, 200)} Safari/537.36\")\n",
    "        \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(search_url)\n",
    "    return driver\n",
    "\n",
    "# Add a delay to mimic human behavior\n",
    "def random_delay(min_seconds=1, max_seconds=3):\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "def get_first_search_result(url, model):\n",
    "    query = extract_domain(url) + \"+\" + model\n",
    "    driver = search_google(query)\n",
    "    \n",
    "    try:\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        link = first_result.get_attribute('href')\n",
    "        return link\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Main function to scrape excavator data\n",
    "def scrape_excavator_data():\n",
    "    url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Create Excel workbook\n",
    "    workbook = openpyxl.Workbook()\n",
    "    table = workbook.active\n",
    "    table.title = \"Excavator Data\"\n",
    "    table.append([\"Model\", \"Info\", \"Detail\"])\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            model_elements = driver.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "\n",
    "            for model, info in tqdm(zip(model_elements), total=len(model_elements), desc=\"Scraping Models\"):\n",
    "                random_delay()\n",
    "                first_result = get_first_search_result(url, model.text)\n",
    "\n",
    "                try:\n",
    "                    tables = pd.read_html(first_result)\n",
    "                    detail_info = tables[1]\n",
    "                    details = detail_info(index=False, header=False)\n",
    "                except Exception as e:\n",
    "                    details = \"Details not found\"\n",
    "\n",
    "                table.append([model.text, details])\n",
    "\n",
    "            try:\n",
    "                load_more_button = driver.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "                ActionChains(driver).move_to_element(load_more_button).perform()\n",
    "                random_delay()\n",
    "                load_more_button.click()\n",
    "                random_delay(2, 4)  # Longer delay for content load\n",
    "            except NoSuchElementException:\n",
    "                print(\"No 'Load More' button. Exiting loop.\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "\n",
    "    file_name = \"excavator_data_with_details(1.2).xlsx\"\n",
    "    workbook.save(file_name)\n",
    "    print(f\"Data successfully saved to {file_name}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_excavator_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST (Bypass Captcha tool used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Initialize the WebDriver for the first website\u001b[39;00m\n\u001b[1;32m     78\u001b[0m options \u001b[38;5;241m=\u001b[39m Options()\n\u001b[0;32m---> 79\u001b[0m b \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Open the target URL\u001b[39;00m\n\u001b[1;32m     82\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://asia.develon-ce.com/en/equipment/excavators/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/chrome/webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[1;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     46\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mDesiredCapabilities\u001b[38;5;241m.\u001b[39mCHROME[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     47\u001b[0m     vendor_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoog\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m     49\u001b[0m     service\u001b[38;5;241m=\u001b[39mservice,\n\u001b[1;32m     50\u001b[0m     keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[1;32m     51\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/chromium/webdriver.py:50\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[1;32m     49\u001b[0m finder \u001b[38;5;241m=\u001b[39m DriverFinder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice, options)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m finder\u001b[38;5;241m.\u001b[39mget_browser_path():\n\u001b[1;32m     51\u001b[0m     options\u001b[38;5;241m.\u001b[39mbinary_location \u001b[38;5;241m=\u001b[39m finder\u001b[38;5;241m.\u001b[39mget_browser_path()\n\u001b[1;32m     52\u001b[0m     options\u001b[38;5;241m.\u001b[39mbrowser_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/common/driver_finder.py:47\u001b[0m, in \u001b[0;36mDriverFinder.get_browser_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_browser_path\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_binary_paths()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowser_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/common/driver_finder.py:67\u001b[0m, in \u001b[0;36mDriverFinder._binary_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     output \u001b[38;5;241m=\u001b[39m SeleniumManager()\u001b[38;5;241m.\u001b[39mbinary_paths(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args())\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Path(output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_path\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/common/selenium_manager.py:55\u001b[0m, in \u001b[0;36mSeleniumManager.binary_paths\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     52\u001b[0m args\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m args\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(args)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/common/selenium_manager.py:119\u001b[0m, in \u001b[0;36mSeleniumManager._run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    117\u001b[0m     completed_proc \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun(args, capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, creationflags\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mCREATE_NO_WINDOW)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     completed_proc \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun(args, capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    120\u001b[0m stdout \u001b[38;5;241m=\u001b[39m completed_proc\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m stderr \u001b[38;5;241m=\u001b[39m completed_proc\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_communicate(\u001b[38;5;28minput\u001b[39m, endtime, timeout)\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:2115\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2109\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2110\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2113\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2115\u001b[0m ready \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium.webdriver import Remote, ChromeOptions\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chromium.remote_connection import ChromiumRemoteConnection\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "AUTH = 'brd-customer-hl_e5bd583b-zone-ai_sraper:n1arzv61qr2a'\n",
    "SBR_WEBDRIVER = f'https://{AUTH}@brd.superproxy.io:9515'\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to search Google\n",
    "def search_google(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    # Set up Chrome options\n",
    "    options = ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(f\"--proxy-server=http://{AUTH.split(':')[0]}\")  # Use proxy\n",
    "    \n",
    "    driver = None  # Initialize driver variable\n",
    "    \n",
    "    try:\n",
    "        # Create a Remote WebDriver instance\n",
    "        driver = Remote(command_executor=SBR_WEBDRIVER, options=options)\n",
    "        driver.get(search_url)\n",
    "        \n",
    "        # Wait for the page to load and return the HTML\n",
    "        time.sleep(3)  # You can use WebDriverWait for better control\n",
    "        html = driver.page_source\n",
    "        return html\n",
    "    \n",
    "    except WebDriverException as e:\n",
    "        print(f\"Error during WebDriver execution: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        # Ensure the driver is quit properly\n",
    "        if driver:\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except Exception as e:\n",
    "                print(f\"Error while quitting WebDriver: {e}\")\n",
    "\n",
    "# Function to get the first search result\n",
    "def get_first_search_result(url, model):\n",
    "    query = extract_domain(url) + \"+\" + model\n",
    "    driver = search_google(query)\n",
    "\n",
    "    if driver is None:\n",
    "        return \"Error: Could not complete Google search.\"\n",
    "\n",
    "    try:\n",
    "        # Find the first search result using the href attribute\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        link = first_result.get_attribute('href')  # Extract the href attribute (the URL)\n",
    "        return link\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        driver.quit()  # Ensure the driver is properly closed\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the WebDriver for the first website\n",
    "options = Options()\n",
    "b = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open the target URL\n",
    "url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "b.get(url)\n",
    "\n",
    "\n",
    "\n",
    "# Create a new Excel workbook and sheet\n",
    "workbook = openpyxl.Workbook()\n",
    "table = workbook.active\n",
    "element = b.find_element(By.CLASS_NAME, \"en_plain_b\")  # Get the WebElement\n",
    "table.title = element.text + \"data\"  # Concatenate its text content with \"data\"\n",
    "\n",
    "# Add headers to the Excel sheet\n",
    "table.append([\"Model\", \"Info\", \"Details\"])\n",
    "\n",
    "# Extract data and insert into the Excel sheet\n",
    "while True:\n",
    "    try:\n",
    "        # Find elements for model names and info\n",
    "        model_elements = b.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "        info_elements = b.find_elements(By.CSS_SELECTOR, \".product_info.en_plain_l\")\n",
    "\n",
    "        # Check if both lists have the same length\n",
    "        if len(model_elements) != len(info_elements):\n",
    "            print(\"Mismatch in number of model and info elements. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        # Add data to the Excel sheet\n",
    "        for model, info in zip(model_elements, info_elements):\n",
    "            # Get the first search result for the model\n",
    "            first_result = get_first_search_result(url, model.text)\n",
    "\n",
    "            # Extract details from the first result\n",
    "            try:\n",
    "                tables = pd.read_html(first_result)\n",
    "                combined_table = pd.concat([tables[0], tables[1]], axis=1)  # Combine tables if needed\n",
    "                combined_table_transposed = combined_table.T  # Transpose the combined table\n",
    "                details = combined_table_transposed\n",
    "            except Exception as e:\n",
    "                details = \"Details not found\"\n",
    "\n",
    "            # Append model, info, and details to the sheet\n",
    "            table.append([model.text, info.text, details])\n",
    "\n",
    "        # Check if a \"Load More\" button exists\n",
    "        try:\n",
    "            load_more_button = b.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "            ActionChains(b).move_to_element(load_more_button).perform()\n",
    "            time.sleep(0.5)\n",
    "            load_more_button.click()\n",
    "            time.sleep(2)  # Wait for content to load\n",
    "        except NoSuchElementException:\n",
    "            print(\"No 'Load More' button found. Exiting loop.\")\n",
    "            break  # Exit loop if no \"Load More\" button is found\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break\n",
    "\n",
    "# Save the workbook to an Excel file\n",
    "file_name = \"excavator_data_with_details.xlsx\"\n",
    "workbook.save(file_name)\n",
    "print(f\"Data successfully saved to {file_name}\")\n",
    "\n",
    "# Close the WebDriver\n",
    "b.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Trial 1.1 & 1.2 & 3.1 & 3.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
