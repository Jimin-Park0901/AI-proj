{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import openpyxl\n",
    "\n",
    "# Initialize the WebDriver\n",
    "options = Options()\n",
    "b = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open the target URL\n",
    "url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "b.get(url)\n",
    "\n",
    "# Create a new Excel workbook and sheet\n",
    "workbook = openpyxl.Workbook()\n",
    "sheet = workbook.active\n",
    "element = b.find_element(By.CLASS_NAME, \"en_plain_b\")  # Get the WebElement\n",
    "sheet.title = element.text + \"data\"  # Concatenate its text content with \"data\"\n",
    "\n",
    "# Add headers to the Excel sheet\n",
    "sheet.append([\"Model\", \"Info\"])\n",
    "\n",
    "# Extract data and insert into the Excel sheet\n",
    "while True:\n",
    "    try:\n",
    "        # Find elements for model names and info\n",
    "        model_elements = b.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "        info_elements = b.find_elements(By.CSS_SELECTOR, \".product_info.en_plain_l\")\n",
    "\n",
    "        # Check if both lists have the same length\n",
    "        if len(model_elements) != len(info_elements):\n",
    "            print(\"Mismatch in number of model and info elements. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        # Add data to the Excel sheet\n",
    "        for model, info, detail in zip(model_elements, info_elements,):\n",
    "            sheet.append([model.text, info.text, detail.text])  # Append model name and info to the sheet\n",
    "\n",
    "        # Check if a \"Load More\" button exists\n",
    "        try:\n",
    "            load_more_button = b.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "            ActionChains(b).move_to_element(load_more_button).perform()\n",
    "            time.sleep(0.5)\n",
    "            load_more_button.click()\n",
    "            time.sleep(2)  # Wait for content to load\n",
    "        except NoSuchElementException:\n",
    "            print(\"No 'Load More' button found. Exiting loop.\")\n",
    "            break  # Exit loop if no \"Load More\" button is found\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break\n",
    "\n",
    "# Save the workbook to an Excel file\n",
    "file_name = \"excavator_data.xlsx\"\n",
    "workbook.save(file_name)\n",
    "print(f\"Data successfully saved to {file_name}\")\n",
    "\n",
    "# Close the WebDriver\n",
    "b.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to search Google\n",
    "def search_google(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    # Set up the Chrome WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")  # Run in headless mode (without opening a browser window)\n",
    "    # options.add_argument(\"--disable-gpu\")\n",
    "    # options.add_argument(\"--no-sandbox\")\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(search_url)\n",
    "    \n",
    "    # Wait for results to load\n",
    "    time.sleep(3)  # Wait for the page to load, adjust if needed\n",
    "    \n",
    "    return driver\n",
    "\n",
    "# Function to get the first search result\n",
    "def get_first_search_result(url, model):\n",
    "    query = extract_domain(url) + \"+\" + model\n",
    "    driver = search_google(query)\n",
    "    \n",
    "    # Find the first search result using the href attribute\n",
    "    try:\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        link = first_result.get_attribute('href')  # Extract the href attribute (the URL)\n",
    "        return link\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        driver.quit()  # Close the browser when done\n",
    "\n",
    "# Initialize the WebDriver for the first website\n",
    "options = Options()\n",
    "b = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open the target URL\n",
    "url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "b.get(url)\n",
    "\n",
    "# Create a new Excel workbook and sheet\n",
    "workbook = openpyxl.Workbook()\n",
    "table = workbook.active\n",
    "element = b.find_element(By.CLASS_NAME, \"en_plain_b\")  # Get the WebElement\n",
    "table.title = element.text + \"data\"  # Concatenate its text content with \"data\"\n",
    "\n",
    "# Add headers to the Excel sheet\n",
    "table.append([\"Model\", \"Info\", \"Details\"])\n",
    "\n",
    "# Extract data and insert into the Excel sheet\n",
    "while True:\n",
    "    try:\n",
    "        # Find elements for model names and info\n",
    "        model_elements = b.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "        info_elements = b.find_elements(By.CSS_SELECTOR, \".product_info.en_plain_l\")\n",
    "\n",
    "        # Check if both lists have the same length\n",
    "        if len(model_elements) != len(info_elements):\n",
    "            print(\"Mismatch in number of model and info elements. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        # Add data to the Excel sheet\n",
    "        for model, info in zip(model_elements, info_elements):\n",
    "            # Get the first search result for the model\n",
    "            first_result = get_first_search_result(url, model.text)\n",
    "\n",
    "            # Extract details from the first result\n",
    "            try:\n",
    "                tables = pd.read_html(first_result)\n",
    "                combined_table = pd.concat([tables[0], tables[1]], axis=1)  # Combine tables if needed\n",
    "                combined_table_transposed = combined_table.T  # Transpose the combined table\n",
    "                details = combined_table_transposed\n",
    "            except Exception as e:\n",
    "                details = \"Details not found\"\n",
    "\n",
    "            # Append model, info, and details to the sheet\n",
    "            table.append([model.text, info.text, details])\n",
    "\n",
    "        # Check if a \"Load More\" button exists\n",
    "        try:\n",
    "            load_more_button = b.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "            ActionChains(b).move_to_element(load_more_button).perform()\n",
    "            time.sleep(0.5)\n",
    "            load_more_button.click()\n",
    "            time.sleep(2)  # Wait for content to load\n",
    "        except NoSuchElementException:\n",
    "            print(\"No 'Load More' button found. Exiting loop.\")\n",
    "            break  # Exit loop if no \"Load More\" button is found\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break\n",
    "\n",
    "# Save the workbook to an Excel file\n",
    "file_name = \"excavator_data_with_details.xlsx\"\n",
    "workbook.save(file_name)\n",
    "print(f\"Data successfully saved to {file_name}\")\n",
    "\n",
    "# Close the WebDriver\n",
    "b.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import time\n",
    "from selenium.webdriver import Remote, ChromeOptions\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chromium.remote_connection import ChromiumRemoteConnection\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "AUTH = 'brd-customer-hl_e5bd583b-zone-ai_sraper:n1arzv61qr2a'\n",
    "SBR_WEBDRIVER = f'https://{AUTH}@brd.superproxy.io:9515'\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to search Google\n",
    "def search_google(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    # Set up Chrome options\n",
    "    options = ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(f\"--proxy-server=http://{AUTH.split(':')[0]}\")  # Use proxy\n",
    "    \n",
    "    driver = None  # Initialize driver variable\n",
    "    \n",
    "    try:\n",
    "        # Create a Remote WebDriver instance\n",
    "        driver = Remote(command_executor=SBR_WEBDRIVER, options=options)\n",
    "        driver.get(search_url)\n",
    "        \n",
    "        # Wait for the page to load and return the HTML\n",
    "        time.sleep(3)  # You can use WebDriverWait for better control\n",
    "        html = driver.page_source\n",
    "        return html\n",
    "    \n",
    "    except WebDriverException as e:\n",
    "        print(f\"Error during WebDriver execution: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        # Ensure the driver is quit properly\n",
    "        if driver:\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except Exception as e:\n",
    "                print(f\"Error while quitting WebDriver: {e}\")\n",
    "\n",
    "# Function to get the first search result\n",
    "def get_first_search_result(url, model):\n",
    "    query = extract_domain(url) + \"+\" + model\n",
    "    driver = search_google(query)\n",
    "\n",
    "    if driver is None:\n",
    "        return \"Error: Could not complete Google search.\"\n",
    "\n",
    "    try:\n",
    "        # Find the first search result using the href attribute\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        link = first_result.get_attribute('href')  # Extract the href attribute (the URL)\n",
    "        return link\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        driver.quit()  # Ensure the driver is properly closed\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the WebDriver for the first website\n",
    "options = Options()\n",
    "b = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open the target URL\n",
    "url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "b.get(url)\n",
    "\n",
    "\n",
    "\n",
    "# Create a new Excel workbook and sheet\n",
    "workbook = openpyxl.Workbook()\n",
    "table = workbook.active\n",
    "element = b.find_element(By.CLASS_NAME, \"en_plain_b\")  # Get the WebElement\n",
    "table.title = element.text + \"data\"  # Concatenate its text content with \"data\"\n",
    "\n",
    "# Add headers to the Excel sheet\n",
    "table.append([\"Model\", \"Info\", \"Details\"])\n",
    "\n",
    "# Extract data and insert into the Excel sheet\n",
    "while True:\n",
    "    try:\n",
    "        # Find elements for model names and info\n",
    "        model_elements = b.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "        info_elements = b.find_elements(By.CSS_SELECTOR, \".product_info.en_plain_l\")\n",
    "\n",
    "        # Check if both lists have the same length\n",
    "        if len(model_elements) != len(info_elements):\n",
    "            print(\"Mismatch in number of model and info elements. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        # Add data to the Excel sheet\n",
    "        for model, info in zip(model_elements, info_elements):\n",
    "            # Get the first search result for the model\n",
    "            first_result = get_first_search_result(url, model.text)\n",
    "\n",
    "            # Extract details from the first result\n",
    "            try:\n",
    "                tables = pd.read_html(first_result)\n",
    "                combined_table = pd.concat([tables[0], tables[1]], axis=1)  # Combine tables if needed\n",
    "                combined_table_transposed = combined_table.T  # Transpose the combined table\n",
    "                details = combined_table_transposed\n",
    "            except Exception as e:\n",
    "                details = \"Details not found\"\n",
    "\n",
    "            # Append model, info, and details to the sheet\n",
    "            table.append([model.text, info.text, details])\n",
    "\n",
    "        # Check if a \"Load More\" button exists\n",
    "        try:\n",
    "            load_more_button = b.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "            ActionChains(b).move_to_element(load_more_button).perform()\n",
    "            time.sleep(0.5)\n",
    "            load_more_button.click()\n",
    "            time.sleep(2)  # Wait for content to load\n",
    "        except NoSuchElementException:\n",
    "            print(\"No 'Load More' button found. Exiting loop.\")\n",
    "            break  # Exit loop if no \"Load More\" button is found\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break\n",
    "\n",
    "# Save the workbook to an Excel file\n",
    "file_name = \"excavator_data_with_details.xlsx\"\n",
    "workbook.save(file_name)\n",
    "print(f\"Data successfully saved to {file_name}\")\n",
    "\n",
    "# Close the WebDriver\n",
    "b.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
