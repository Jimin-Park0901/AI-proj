{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic -> \"excavator_data.xlsx\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detail한 정보는 없음. (Just Model and Info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'Load More' button. Exiting loop.\n",
      "Data saved to excavator_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urlparse\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to set up the WebDriver with anti-detection measures\n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    # Randomized User-Agent\n",
    "    options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT {random.randint(6, 10)}.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(90, 110)}.0.4472.{random.randint(100, 200)} Safari/537.36\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "# Function to add random delay\n",
    "def random_delay(min_seconds=1, max_seconds=3):\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "# Main scraping logic\n",
    "def scrape_excavator_data():\n",
    "    url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "    driver = setup_driver()\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Create Excel workbook\n",
    "    workbook = openpyxl.Workbook()\n",
    "    sheet = workbook.active\n",
    "    sheet.title = \"Excavator Data\"\n",
    "    sheet.append([\"Model\", \"Info\", \"Details\"])\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Extract models and info\n",
    "            model_elements = driver.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "            info_elements = driver.find_elements(By.CSS_SELECTOR, \".product_info.en_plain_l\")\n",
    "            \n",
    "            if len(model_elements) != len(info_elements):\n",
    "                print(\"Mismatch in data lengths. Exiting.\")\n",
    "                break\n",
    "            \n",
    "            for model, info in zip(model_elements, info_elements):\n",
    "                # Simulate human-like delay\n",
    "                random_delay()\n",
    "                sheet.append([model.text, info.text, \"Details Placeholder\"])  # Placeholder for details\n",
    "                \n",
    "            # Click 'Load More' button\n",
    "            try:\n",
    "                load_more_button = driver.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "                ActionChains(driver).move_to_element(load_more_button).perform()\n",
    "                random_delay()\n",
    "                load_more_button.click()\n",
    "                random_delay(2, 4)  # Longer delay for content load\n",
    "            except NoSuchElementException:\n",
    "                print(\"No 'Load More' button. Exiting loop.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "    \n",
    "    # Save data to Excel\n",
    "    file_name = \"excavator_data.xlsx\"\n",
    "    workbook.save(file_name)\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "    \n",
    "    # Close browser\n",
    "    driver.quit()\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_excavator_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type1 (첫번째에는 잘 작동됐는데 captcha문제로 지금은 안됨.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Cannot convert                               0                             1   \\\n",
      "Classification  Operating Weight  Bucket Capacity (SAE HEAPED)   \n",
      "DX89R-7                 9,518 kg                       0.28 m³   \n",
      "\n",
      "                                           2             3             4   \\\n",
      "Classification  Engine Power (SAE j1349, net)  Engine Model  Travel Speed   \n",
      "DX89R-7                   48.5/2,100 (kW/rpm)   Develon D24  2.7/4.7 km/h   \n",
      "\n",
      "                                5                   6                    7   \\\n",
      "Classification  Max. Digging Reach  Max. Digging Depth  Max. Digging Height   \n",
      "DX89R-7                   7,015 mm            4,110 mm             7,030 mm   \n",
      "\n",
      "                            8              9               10  \n",
      "Classification  Overall Length  Overall Width  Overall Height  \n",
      "DX89R-7               6,430 mm       2,250 mm        2,657 mm   to Excel\n",
      "Data successfully saved to excavator_data_with_details(0).xlsx\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to search Google\n",
    "def search_google(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    # Set up the Chrome WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")  # Run in headless mode (without opening a browser window)\n",
    "    # options.add_argument(\"--disable-gpu\")\n",
    "    # options.add_argument(\"--no-sandbox\")\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(search_url)\n",
    "    \n",
    "    # Wait for results to load\n",
    "    time.sleep(3)  # Wait for the page to load, adjust if needed\n",
    "    \n",
    "    return driver\n",
    "\n",
    "# Function to get the first search result\n",
    "def get_first_search_result(url, model):\n",
    "    query = extract_domain(url) + \"+\" + model\n",
    "    driver = search_google(query)\n",
    "    \n",
    "    # Find the first search result using the href attribute\n",
    "    try:\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        link = first_result.get_attribute('href')  # Extract the href attribute (the URL)\n",
    "        return link\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        driver.quit()  # Close the browser when done\n",
    "\n",
    "# Initialize the WebDriver for the first website\n",
    "options = Options()\n",
    "b = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open the target URL\n",
    "url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "b.get(url)\n",
    "\n",
    "# Create a new Excel workbook and sheet\n",
    "workbook = openpyxl.Workbook()\n",
    "table = workbook.active\n",
    "element = b.find_element(By.CLASS_NAME, \"en_plain_b\")  # Get the WebElement\n",
    "table.title = element.text + \"data\"  # Concatenate its text content with \"data\"\n",
    "\n",
    "# Add headers to the Excel sheet\n",
    "table.append([\"Model\", \"Info\", \"Details\"])\n",
    "\n",
    "# Extract data and insert into the Excel sheet\n",
    "while True:\n",
    "    try:\n",
    "        # Find elements for model names and info\n",
    "        model_elements = b.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "        info_elements = b.find_elements(By.CSS_SELECTOR, \".product_info.en_plain_l\")\n",
    "\n",
    "        # Check if both lists have the same length\n",
    "        if len(model_elements) != len(info_elements):\n",
    "            print(\"Mismatch in number of model and info elements. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        # Add data to the Excel sheet\n",
    "        for model, info in zip(model_elements, info_elements):\n",
    "            # Get the first search result for the model\n",
    "            first_result = get_first_search_result(url, model.text)\n",
    "\n",
    "            # Extract details from the first result\n",
    "            try:\n",
    "                tables = pd.read_html(first_result)\n",
    "                combined_table = pd.concat([tables[0], tables[1]], axis=1)  # Combine tables if needed\n",
    "                combined_table_transposed = combined_table.T  # Transpose the combined table\n",
    "                details = combined_table_transposed\n",
    "            except Exception as e:\n",
    "                details = \"Details not found\"\n",
    "\n",
    "            # Append model, info, and details to the sheet\n",
    "            table.append([model.text, info.text, details])\n",
    "\n",
    "        # Check if a \"Load More\" button exists\n",
    "        try:\n",
    "            load_more_button = b.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "            ActionChains(b).move_to_element(load_more_button).perform()\n",
    "            time.sleep(0.5)\n",
    "            load_more_button.click()\n",
    "            time.sleep(2)  # Wait for content to load\n",
    "        except NoSuchElementException:\n",
    "            print(\"No 'Load More' button found. Exiting loop.\")\n",
    "            break  # Exit loop if no \"Load More\" button is found\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break\n",
    "\n",
    "# Save the workbook to an Excel file\n",
    "file_name = \"excavator_data_with_details(0).xlsx\"\n",
    "workbook.save(file_name)\n",
    "print(f\"Data successfully saved to {file_name}\")\n",
    "\n",
    "# Close the WebDriver\n",
    "b.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type2 -> \"excator_data_with_link.xlsx\" (지금은 링크만 존재) (40~60min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type1의 captcha 해결 버전"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Kept failing so I checked whether if it uses correct url and apparently google blocks after certain usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'Load More' button. Exiting loop.\n",
      "Data saved to excavator_data_with_details.xlsx\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urlparse\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import openpyxl\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to set up the WebDriver with anti-detection measures\n",
    "def setup_driver(headless=False):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    # Randomized User-Agent\n",
    "    options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT {random.randint(6, 10)}.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(90, 110)}.0.4472.{random.randint(100, 200)} Safari/537.36\")\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "# Function to add random delay\n",
    "def random_delay(min_seconds=1, max_seconds=3):\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "# Function to search Google and get the first result\n",
    "def search_google(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    driver = setup_driver(headless=True)\n",
    "    driver.get(search_url)\n",
    "    random_delay(2, 4)\n",
    "    try:\n",
    "        # Find the first search result\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        link = first_result.get_attribute('href')  # Extract the href attribute (the URL)\n",
    "        return link\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Main scraping logic\n",
    "def scrape_excavator_data():\n",
    "    url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "    driver = setup_driver()\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Create Excel workbook\n",
    "    workbook = openpyxl.Workbook()\n",
    "    sheet = workbook.active\n",
    "    sheet.title = \"Excavator Data\"\n",
    "    sheet.append([\"Model\", \"Info\", \"Details\"])\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Extract models and info\n",
    "            model_elements = driver.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "            info_elements = driver.find_elements(By.CSS_SELECTOR, \".product_info.en_plain_l\")\n",
    "            \n",
    "            if len(model_elements) != len(info_elements):\n",
    "                print(\"Mismatch in data lengths. Exiting.\")\n",
    "                break\n",
    "            \n",
    "            for model, info in zip(model_elements, info_elements):\n",
    "                random_delay()\n",
    "                model_text = model.text\n",
    "                info_text = info.text\n",
    "                # Search Google for additional details\n",
    "                query = extract_domain(url) + \"+\" + model_text\n",
    "                details = search_google(query)\n",
    "                sheet.append([model_text, info_text, details])\n",
    "            \n",
    "            # Click 'Load More' button\n",
    "            try:\n",
    "                load_more_button = driver.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "                ActionChains(driver).move_to_element(load_more_button).perform()\n",
    "                random_delay()\n",
    "                load_more_button.click()\n",
    "                random_delay(2, 4)  # Longer delay for content load\n",
    "            except NoSuchElementException:\n",
    "                print(\"No 'Load More' button. Exiting loop.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "\n",
    "    # Save data to Excel\n",
    "    file_name = \"excavator_data_with_details.xlsx\"\n",
    "    workbook.save(file_name)\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "    \n",
    "    # Close browser\n",
    "    driver.quit()\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_excavator_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Models: 100%|██████████| 24/24 [02:27<00:00,  6.16s/it]\n",
      "Scraping Models:   0%|          | 0/29 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m     driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     scrape_excavator_data()\n",
      "Cell \u001b[0;32mIn[29], line 77\u001b[0m, in \u001b[0;36mscrape_excavator_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m first_result \u001b[38;5;241m=\u001b[39m get_first_search_result(url, model\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     tables \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_html(first_result)\n\u001b[1;32m     78\u001b[0m     detail_info \u001b[38;5;241m=\u001b[39m tables[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Assuming details are in the second table\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     flattened_details \u001b[38;5;241m=\u001b[39m detail_info\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Flatten DataFrame into a single list\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/html.py:1240\u001b[0m, in \u001b[0;36mread_html\u001b[0;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   1225\u001b[0m     [\n\u001b[1;32m   1226\u001b[0m         is_file_like(io),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1230\u001b[0m     ]\n\u001b[1;32m   1231\u001b[0m ):\n\u001b[1;32m   1232\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal html to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_html\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1234\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\n\u001b[1;32m   1241\u001b[0m     flavor\u001b[38;5;241m=\u001b[39mflavor,\n\u001b[1;32m   1242\u001b[0m     io\u001b[38;5;241m=\u001b[39mio,\n\u001b[1;32m   1243\u001b[0m     match\u001b[38;5;241m=\u001b[39mmatch,\n\u001b[1;32m   1244\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   1245\u001b[0m     index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[1;32m   1246\u001b[0m     skiprows\u001b[38;5;241m=\u001b[39mskiprows,\n\u001b[1;32m   1247\u001b[0m     parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[1;32m   1248\u001b[0m     thousands\u001b[38;5;241m=\u001b[39mthousands,\n\u001b[1;32m   1249\u001b[0m     attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1250\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   1251\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   1252\u001b[0m     converters\u001b[38;5;241m=\u001b[39mconverters,\n\u001b[1;32m   1253\u001b[0m     na_values\u001b[38;5;241m=\u001b[39mna_values,\n\u001b[1;32m   1254\u001b[0m     keep_default_na\u001b[38;5;241m=\u001b[39mkeep_default_na,\n\u001b[1;32m   1255\u001b[0m     displayed_only\u001b[38;5;241m=\u001b[39mdisplayed_only,\n\u001b[1;32m   1256\u001b[0m     extract_links\u001b[38;5;241m=\u001b[39mextract_links,\n\u001b[1;32m   1257\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1258\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   1259\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/html.py:983\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    972\u001b[0m p \u001b[38;5;241m=\u001b[39m parser(\n\u001b[1;32m    973\u001b[0m     io,\n\u001b[1;32m    974\u001b[0m     compiled_match,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    979\u001b[0m     storage_options,\n\u001b[1;32m    980\u001b[0m )\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 983\u001b[0m     tables \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mparse_tables()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseekable\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io\u001b[38;5;241m.\u001b[39mseekable():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/html.py:249\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_tables(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_doc(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs)\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/html.py:785\u001b[0m, in \u001b[0;36m_LxmlFrameParser._build_doc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio):\n\u001b[0;32m--> 785\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    786\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options\n\u001b[1;32m    787\u001b[0m         ) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    788\u001b[0m             r \u001b[38;5;241m=\u001b[39m parse(f\u001b[38;5;241m.\u001b[39mhandle, parser\u001b[38;5;241m=\u001b[39mparser)\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# try to parse the input in the simplest way\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m _get_filepath_or_buffer(\n\u001b[1;32m    729\u001b[0m     path_or_buf,\n\u001b[1;32m    730\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m    731\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    732\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    733\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    734\u001b[0m )\n\u001b[1;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    737\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:384\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    383\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urlopen(req_info) \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    385\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:289\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/urllib/request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/urllib/request.py:515\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    512\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    514\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 515\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(req, data)\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    518\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/urllib/request.py:532\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    531\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 532\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_open, protocol, protocol \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    533\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m'\u001b[39m, req)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/urllib/request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/urllib/request.py:1392\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_open(http\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mHTTPSConnection, req,\n\u001b[1;32m   1393\u001b[0m                         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/urllib/request.py:1344\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1343\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1344\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[1;32m   1345\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1336\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{}, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   1334\u001b[0m             encode_chunked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1382\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders(body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1091\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1089\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer)\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1091\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1094\u001b[0m \n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1097\u001b[0m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1035\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m-> 1035\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1470\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1467\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1468\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnect to a host on a given (SSL) port.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1470\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m   1473\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1001\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n\u001b[0;32m-> 1001\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection(\n\u001b[1;32m   1002\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address)\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/socket.py:838\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m    837\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m--> 838\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m    840\u001b[0m exceptions\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urlparse\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to search Google\n",
    "def search_google(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT {random.randint(6, 10)}.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(90, 110)}.0.4472.{random.randint(100, 200)} Safari/537.36\")\n",
    "        \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(search_url)\n",
    "    return driver\n",
    "\n",
    "# Add a delay to mimic human behavior\n",
    "def random_delay(min_seconds=1, max_seconds=3):\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "def get_first_search_result(url, model):\n",
    "    query = extract_domain(url) + \"+\" + model\n",
    "    driver = search_google(query)\n",
    "    \n",
    "    try:\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        link = first_result.get_attribute('href')\n",
    "        return link\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Main function to scrape excavator data\n",
    "def scrape_excavator_data():\n",
    "    url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Create Excel workbook\n",
    "    workbook = openpyxl.Workbook()\n",
    "    table = workbook.active\n",
    "    table.title = \"Excavator Data\"\n",
    "    table.append([\"Model\"] + [f\"Detail {i}\" for i in range(1, 20)])  # Header row (adjust number of details as needed)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            model_elements = driver.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "\n",
    "            for model in tqdm(model_elements, total=len(model_elements), desc=\"Scraping Models\"):\n",
    "                random_delay()\n",
    "                first_result = get_first_search_result(url, model.text)\n",
    "\n",
    "                try:\n",
    "                    tables = pd.read_html(first_result)\n",
    "                    detail_info = tables[1]  # Assuming details are in the second table\n",
    "                    flattened_details = detail_info.values.flatten().tolist()  # Flatten DataFrame into a single list\n",
    "                except Exception as e:\n",
    "                    flattened_details = [\"Details not found\"]\n",
    "\n",
    "                # Append model and flattened details in one row\n",
    "                table.append([model.text] + flattened_details)\n",
    "            \n",
    "            try:\n",
    "                load_more_button = driver.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "                ActionChains(driver).move_to_element(load_more_button).perform()\n",
    "                random_delay()\n",
    "                load_more_button.click()\n",
    "                random_delay(2, 4)  # Longer delay for content load\n",
    "            except NoSuchElementException:\n",
    "                print(\"No 'Load More' button. Exiting loop.\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "\n",
    "    file_name = \"excavator_data_with_details.xlsx\"\n",
    "    workbook.save(file_name)\n",
    "    print(f\"Data successfully saved to {file_name}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_excavator_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 보험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'setup_driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 119\u001b[0m\n\u001b[1;32m    116\u001b[0m     driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m     scrape_excavator_data()\n",
      "Cell \u001b[0;32mIn[17], line 61\u001b[0m, in \u001b[0;36mscrape_excavator_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_excavator_data\u001b[39m():\n\u001b[1;32m     60\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://asia.develon-ce.com/en/equipment/excavators/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 61\u001b[0m     driver \u001b[38;5;241m=\u001b[39m setup_driver()\n\u001b[1;32m     62\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Create Excel workbook\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'setup_driver' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urlparse\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to set up the WebDriver with anti-detection measures\n",
    "\n",
    "def search_google(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    # Randomized User-Agent\n",
    "    options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT {random.randint(6, 10)}.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(90, 110)}.0.4472.{random.randint(100, 200)} Safari/537.36\")\n",
    "        \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(search_url)\n",
    "\n",
    "    return driver\n",
    "    \n",
    "\n",
    "def random_delay(min_seconds=1, max_seconds=3):\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "def get_first_search_result(url, model):\n",
    "    query = extract_domain(url) + \"+\" + model\n",
    "    driver = search_google(query)\n",
    "    \n",
    "    # Find the first search result using the href attribute\n",
    "    try:\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        link = first_result.get_attribute('href')  # Extract the href attribute (the URL)\n",
    "        return link\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        driver.quit()  # Close the browser when done\n",
    "\n",
    "# Function to scrape excavator data\n",
    "def scrape_excavator_data():\n",
    "    url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "    driver = setup_driver()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Create Excel workbook\n",
    "    workbook = openpyxl.Workbook()\n",
    "    table = workbook.active\n",
    "    table.title = \"Excavator Data\"\n",
    "    table.append([\"Model\", \"Info\", \"Detail\"])\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Extract models and info\n",
    "            model_elements = driver.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "            info_elements = driver.find_elements(By.CSS_SELECTOR, \".product_info.en_plain_l\")\n",
    "\n",
    "            if len(model_elements) != len(info_elements):\n",
    "                print(\"Mismatch in data lengths. Exiting.\")\n",
    "                break\n",
    "\n",
    "            for model, info in zip(model_elements, info_elements):\n",
    "                random_delay()\n",
    "                first_result = get_first_search_result(url,model.text)\n",
    "\n",
    "                try:\n",
    "                    tables = pd.read_html(first_result)\n",
    "                    # detail_title_raw = tables[0]\n",
    "                    # detail_title_transposed = detail_title_raw.T\n",
    "                    # detail_title = detail_title_transposed\n",
    "\n",
    "                    detail_info = tables[1]\n",
    "                    detail_info_transposed = detail_info.T\n",
    "                    details = detail_info_transposed.to_string(index=False, header=False)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    details = \"Details not found\"\n",
    "                \n",
    "                table.append([model.text, info.text, details])\n",
    "\n",
    "            try:\n",
    "                load_more_button = driver.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "                ActionChains(driver).move_to_element(load_more_button).perform()\n",
    "                random_delay()\n",
    "                load_more_button.click()\n",
    "                random_delay(2, 4)  # Longer delay for content load\n",
    "            except NoSuchElementException:\n",
    "                print(\"No 'Load More' button. Exiting loop.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "\n",
    "    file_name = \"excavator_data_with_details(2).xlsx\"\n",
    "    workbook.save(file_name)\n",
    "    print(f\"Data successfully saved to {file_name}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_excavator_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST (Bypass Captcha tool used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Initialize the WebDriver for the first website\u001b[39;00m\n\u001b[1;32m     78\u001b[0m options \u001b[38;5;241m=\u001b[39m Options()\n\u001b[0;32m---> 79\u001b[0m b \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Open the target URL\u001b[39;00m\n\u001b[1;32m     82\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://asia.develon-ce.com/en/equipment/excavators/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/chrome/webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[1;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     46\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mDesiredCapabilities\u001b[38;5;241m.\u001b[39mCHROME[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     47\u001b[0m     vendor_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoog\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m     49\u001b[0m     service\u001b[38;5;241m=\u001b[39mservice,\n\u001b[1;32m     50\u001b[0m     keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[1;32m     51\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/chromium/webdriver.py:50\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[1;32m     49\u001b[0m finder \u001b[38;5;241m=\u001b[39m DriverFinder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice, options)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m finder\u001b[38;5;241m.\u001b[39mget_browser_path():\n\u001b[1;32m     51\u001b[0m     options\u001b[38;5;241m.\u001b[39mbinary_location \u001b[38;5;241m=\u001b[39m finder\u001b[38;5;241m.\u001b[39mget_browser_path()\n\u001b[1;32m     52\u001b[0m     options\u001b[38;5;241m.\u001b[39mbrowser_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/common/driver_finder.py:47\u001b[0m, in \u001b[0;36mDriverFinder.get_browser_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_browser_path\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_binary_paths()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowser_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/common/driver_finder.py:67\u001b[0m, in \u001b[0;36mDriverFinder._binary_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     output \u001b[38;5;241m=\u001b[39m SeleniumManager()\u001b[38;5;241m.\u001b[39mbinary_paths(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args())\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Path(output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_path\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/common/selenium_manager.py:55\u001b[0m, in \u001b[0;36mSeleniumManager.binary_paths\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     52\u001b[0m args\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m args\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(args)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/common/selenium_manager.py:119\u001b[0m, in \u001b[0;36mSeleniumManager._run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    117\u001b[0m     completed_proc \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun(args, capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, creationflags\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mCREATE_NO_WINDOW)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     completed_proc \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun(args, capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    120\u001b[0m stdout \u001b[38;5;241m=\u001b[39m completed_proc\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m stderr \u001b[38;5;241m=\u001b[39m completed_proc\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_communicate(\u001b[38;5;28minput\u001b[39m, endtime, timeout)\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:2115\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2109\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2110\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2113\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2115\u001b[0m ready \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium.webdriver import Remote, ChromeOptions\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chromium.remote_connection import ChromiumRemoteConnection\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "AUTH = 'brd-customer-hl_e5bd583b-zone-ai_sraper:n1arzv61qr2a'\n",
    "SBR_WEBDRIVER = f'https://{AUTH}@brd.superproxy.io:9515'\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to search Google\n",
    "def search_google(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    \n",
    "    # Set up Chrome options\n",
    "    options = ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(f\"--proxy-server=http://{AUTH.split(':')[0]}\")  # Use proxy\n",
    "    \n",
    "    driver = None  # Initialize driver variable\n",
    "    \n",
    "    try:\n",
    "        # Create a Remote WebDriver instance\n",
    "        driver = Remote(command_executor=SBR_WEBDRIVER, options=options)\n",
    "        driver.get(search_url)\n",
    "        \n",
    "        # Wait for the page to load and return the HTML\n",
    "        time.sleep(3)  # You can use WebDriverWait for better control\n",
    "        html = driver.page_source\n",
    "        return html\n",
    "    \n",
    "    except WebDriverException as e:\n",
    "        print(f\"Error during WebDriver execution: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        # Ensure the driver is quit properly\n",
    "        if driver:\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except Exception as e:\n",
    "                print(f\"Error while quitting WebDriver: {e}\")\n",
    "\n",
    "# Function to get the first search result\n",
    "def get_first_search_result(url, model):\n",
    "    query = extract_domain(url) + \"+\" + model\n",
    "    driver = search_google(query)\n",
    "\n",
    "    if driver is None:\n",
    "        return \"Error: Could not complete Google search.\"\n",
    "\n",
    "    try:\n",
    "        # Find the first search result using the href attribute\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        link = first_result.get_attribute('href')  # Extract the href attribute (the URL)\n",
    "        return link\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        driver.quit()  # Ensure the driver is properly closed\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the WebDriver for the first website\n",
    "options = Options()\n",
    "b = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open the target URL\n",
    "url = \"https://asia.develon-ce.com/en/equipment/excavators/\"\n",
    "b.get(url)\n",
    "\n",
    "\n",
    "\n",
    "# Create a new Excel workbook and sheet\n",
    "workbook = openpyxl.Workbook()\n",
    "table = workbook.active\n",
    "element = b.find_element(By.CLASS_NAME, \"en_plain_b\")  # Get the WebElement\n",
    "table.title = element.text + \"data\"  # Concatenate its text content with \"data\"\n",
    "\n",
    "# Add headers to the Excel sheet\n",
    "table.append([\"Model\", \"Info\", \"Details\"])\n",
    "\n",
    "# Extract data and insert into the Excel sheet\n",
    "while True:\n",
    "    try:\n",
    "        # Find elements for model names and info\n",
    "        model_elements = b.find_elements(By.CSS_SELECTOR, \".product_name.en_plain_b\")\n",
    "        info_elements = b.find_elements(By.CSS_SELECTOR, \".product_info.en_plain_l\")\n",
    "\n",
    "        # Check if both lists have the same length\n",
    "        if len(model_elements) != len(info_elements):\n",
    "            print(\"Mismatch in number of model and info elements. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        # Add data to the Excel sheet\n",
    "        for model, info in zip(model_elements, info_elements):\n",
    "            # Get the first search result for the model\n",
    "            first_result = get_first_search_result(url, model.text)\n",
    "\n",
    "            # Extract details from the first result\n",
    "            try:\n",
    "                tables = pd.read_html(first_result)\n",
    "                combined_table = pd.concat([tables[0], tables[1]], axis=1)  # Combine tables if needed\n",
    "                combined_table_transposed = combined_table.T  # Transpose the combined table\n",
    "                details = combined_table_transposed\n",
    "            except Exception as e:\n",
    "                details = \"Details not found\"\n",
    "\n",
    "            # Append model, info, and details to the sheet\n",
    "            table.append([model.text, info.text, details])\n",
    "\n",
    "        # Check if a \"Load More\" button exists\n",
    "        try:\n",
    "            load_more_button = b.find_element(By.CLASS_NAME, \"btn_more\")\n",
    "            ActionChains(b).move_to_element(load_more_button).perform()\n",
    "            time.sleep(0.5)\n",
    "            load_more_button.click()\n",
    "            time.sleep(2)  # Wait for content to load\n",
    "        except NoSuchElementException:\n",
    "            print(\"No 'Load More' button found. Exiting loop.\")\n",
    "            break  # Exit loop if no \"Load More\" button is found\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break\n",
    "\n",
    "# Save the workbook to an Excel file\n",
    "file_name = \"excavator_data_with_details.xlsx\"\n",
    "workbook.save(file_name)\n",
    "print(f\"Data successfully saved to {file_name}\")\n",
    "\n",
    "# Close the WebDriver\n",
    "b.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Trial 1.1 & 1.2 & 3.1 & 3.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
